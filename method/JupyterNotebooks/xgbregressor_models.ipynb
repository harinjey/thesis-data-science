{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regressor: Default & Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"xgb_tuned.joblib\"\n",
    "#xgb_tuned = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "frag = pd.read_csv(\"/mnt/project/Transpose/hidra/results/salmon-hidra/batch1/fragment_filtered_fasta/filtered_fragments.fasta\", sep=\"\\t\", header= None,  names=[\"fragment_id\", \"sequence\"])\n",
    "\n",
    "deseq = pd.read_csv(\"/mnt/project/Transpose/hidra/results/salmon-hidra/batch1/DESeq_TSV/DESeq_res.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data for n nucleotides\n",
    "frag = frag.loc[~frag['sequence'].str.contains('n')].reset_index()\n",
    "frag.drop(\"index\", axis=1)\n",
    "\n",
    "deseq = deseq.drop(labels = [2634852, 2634853, 2634854, 2634855, 2634856]).reset_index()\n",
    "deseq = deseq.drop(\"index\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non significant values are given 1 as pvalue and padj\n",
    "\n",
    "not_sig = {\"pvalue\": 1, \"padj\":1}\n",
    "\n",
    "deseq = deseq.fillna(value=not_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating frag and deseq tables to one table\n",
    "df_all_cols = pd.concat([frag, deseq ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting fragments from chromosome 21 and 25 form the table and create a dataframe with only those as test set\n",
    "chrom_21 = df_all_cols[df_all_cols['fragment'].str.contains('21:')]\n",
    "\n",
    "chrom_25 = df_all_cols[df_all_cols['fragment'].str.contains('25:')]\n",
    "\n",
    "test_set = pd.concat([chrom_21, chrom_25], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set of all fragments except from chromosome 21 and 25\n",
    "df_training = df_all_cols.loc[~df_all_cols[\"fragment\"].str.contains('25:') & ~df_all_cols[\"fragment\"].str.contains('21:')].reset_index()\n",
    "df_training.drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select fragments within top 10% basemean for both training and test sets\n",
    "\n",
    "filtered_frag = df_training[df_training[\"baseMean\"] > 62.609201]\n",
    "\n",
    "filtered_test = test_set[test_set[\"baseMean\"] > 65.333982]\n",
    "\n",
    "# Extracting the log2FoldChange values (true values) from training set and test set\n",
    "y_test = (filtered_test.log2FoldChange)\n",
    "y = (filtered_frag.log2FoldChange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_mer function to create k-mer features\n",
    "# Modified from: k-mer function modified from: https://www.kaggle.com/code/mohdmuttalib/biological-sequence-modeling-with-k-mer-features\n",
    "def getKmers(sequence, size):\n",
    "    kmer_seq= [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]\n",
    "    return ' '.join(kmer_seq)\n",
    "#from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-mer encoding with k-mer sizes from 2 to 6 for each fragment in test set\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "filtered_test[\"kmer_2\"] = filtered_test[\"sequence\"].apply(lambda x: getKmers(x, size=2))\n",
    "kmer2test = vectorizer.fit_transform(filtered_test['kmer_2']).astype('float32')\n",
    "kmer_2test = np.asarray(kmer2test/kmer2test.sum(axis=1))\n",
    "kmer2_testdf = pd.DataFrame(kmer_2test,columns=['2_mer'] * (4**2))\n",
    "\n",
    "filtered_test[\"kmer_3\"] = filtered_test[\"sequence\"].apply(lambda x: getKmers(x, size=3))\n",
    "kmer3test = vectorizer.fit_transform(filtered_test['kmer_3']).astype('float32')\n",
    "kmer_3test = np.asarray(kmer3test/kmer3test.sum(axis=1))\n",
    "kmer3_testdf = pd.DataFrame(kmer_3test,columns=['3_mer'] * (4**3))\n",
    "\n",
    "filtered_test[\"kmer_4\"] = filtered_test[\"sequence\"].apply(lambda x: getKmers(x, size=4))\n",
    "kmer4test = vectorizer.fit_transform(filtered_test['kmer_4']).astype('float32')\n",
    "kmer_4test = np.asarray(kmer4test/kmer4test.sum(axis=1))\n",
    "kmer4_testdf = pd.DataFrame(kmer_4test,columns=['4_mer'] * (4**4))\n",
    "\n",
    "filtered_test[\"kmer_5\"] = filtered_test[\"sequence\"].apply(lambda x: getKmers(x, size=5))\n",
    "kmer5test = vectorizer.fit_transform(filtered_test['kmer_5']).astype('float32')\n",
    "kmer_5test = np.asarray(kmer5test/kmer5test.sum(axis=1))\n",
    "kmer5_testdf = pd.DataFrame(kmer_5test,columns=['5_mer'] * (4**5))\n",
    "\n",
    "filtered_test[\"kmer_6\"] = filtered_test[\"sequence\"].apply(lambda x: getKmers(x, size=6))\n",
    "kmer6test = vectorizer.fit_transform(filtered_test['kmer_6']).astype('float32')\n",
    "kmer_6test = np.asarray(kmer6test/kmer6test.sum(axis=1))\n",
    "kmer6_testdf = pd.DataFrame(kmer_6test,columns=['6_mer'] * (4**6))\n",
    "\n",
    "X_test = pd.concat([kmer2_testdf, kmer3_testdf, kmer4_testdf, kmer5_testdf, kmer6_testdf], axis = 1)\n",
    "\n",
    "X_test= np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-mer features for training set\n",
    "vectorizer2 = CountVectorizer()\n",
    "filtered_frag[\"kmer_2\"] = filtered_frag[\"sequence\"].apply(lambda x: getKmers(x, size=2))\n",
    "kmer2 = vectorizer2.fit_transform(filtered_frag['kmer_2']).astype('float32')\n",
    "kmer_2 = np.asarray(kmer2/kmer2.sum(axis=1))\n",
    "kmer2_df = pd.DataFrame(kmer_2,columns=['2_mer'] * (4**2))\n",
    "feature_names1 = vectorizer2.get_feature_names_out()\n",
    "\n",
    "\n",
    "vectorizer3 = CountVectorizer()\n",
    "filtered_frag[\"kmer_3\"] = filtered_frag[\"sequence\"].apply(lambda x: getKmers(x, size=3))\n",
    "kmer3 = vectorizer3.fit_transform(filtered_frag['kmer_3']).astype('float32')\n",
    "kmer_3 = np.asarray(kmer3/kmer3.sum(axis=1))\n",
    "kmer3_df = pd.DataFrame(kmer_3,columns=['3_mer'] * (4**3))\n",
    "feature_names2 = vectorizer3.get_feature_names_out()\n",
    "\n",
    "vectorizer4 = CountVectorizer()\n",
    "filtered_frag[\"kmer_4\"] = filtered_frag[\"sequence\"].apply(lambda x: getKmers(x, size=4))\n",
    "kmer4 = vectorizer4.fit_transform(filtered_frag['kmer_4']).astype('float32')\n",
    "kmer_4 = np.asarray(kmer4/kmer4.sum(axis=1))\n",
    "kmer4_df = pd.DataFrame(kmer_4,columns=['4_mer'] * (4**4))\n",
    "feature_names3 = vectorizer4.get_feature_names_out()\n",
    "\n",
    "vectorizer5 = CountVectorizer()\n",
    "filtered_frag[\"kmer_5\"] = filtered_frag[\"sequence\"].apply(lambda x: getKmers(x, size=5))\n",
    "kmer5 = vectorizer5.fit_transform(filtered_frag['kmer_5']).astype('float32')\n",
    "kmer_5 = np.asarray(kmer5/kmer5.sum(axis=1))\n",
    "kmer5_df = pd.DataFrame(kmer_5,columns=['5_mer'] * (4**5))\n",
    "feature_names4 = vectorizer5.get_feature_names_out()\n",
    "\n",
    "vectorizer6 = CountVectorizer()\n",
    "filtered_frag[\"kmer_6\"] = filtered_frag[\"sequence\"].apply(lambda x: getKmers(x, size=6))\n",
    "kmer6 = vectorizer6.fit_transform(filtered_frag['kmer_6']).astype('float32')\n",
    "kmer_6 = np.asarray(kmer6/kmer6.sum(axis=1))\n",
    "kmer6_df = pd.DataFrame(kmer_6,columns=['6_mer'] * (4**6))\n",
    "feature_names5 = vectorizer6.get_feature_names_out()\n",
    "\n",
    "\n",
    "X = pd.concat([kmer2_df, kmer3_df, kmer4_df, kmer5_df, kmer6_df], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature names to use for feature importance\n",
    "feature_names = []\n",
    "feature_names.extend(feature_names1)\n",
    "feature_names.extend(feature_names2)\n",
    "feature_names.extend(feature_names3)\n",
    "feature_names.extend(feature_names4)\n",
    "feature_names.extend(feature_names5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names as list \n",
    "X_names = list(X.columns)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, validation split\n",
    "X_train,X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model xgboost regressor with default parameter\n",
    "clf_xgboost = xgb.XGBRegressor(objective=\"reg:squarederror\",random_state=42)\n",
    "\n",
    "# Train the model \n",
    "\n",
    "clf_xgboost.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get predictions\n",
    "y_pred = clf_xgboost.predict(X_valid)\n",
    "\n",
    "# Calculatig rmse\n",
    "rmse_pred = mean_squared_error(y_valid, y_pred) \n",
    "\n",
    "print(\"Root Mean Squared Error, validation:\" , np.sqrt(rmse_pred))\n",
    "\n",
    "print(\"R2 score validation:\" , r2_score(y_valid, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set \n",
    "y_pred_test = clf_xgboost.predict(X_test)\n",
    "\n",
    "# Calculatig root mean squared error\n",
    "rmse_pred_test = mean_squared_error(y_test, y_pred_test) \n",
    "\n",
    "print(\"Root Mean Squared Error, test:\" , np.sqrt(rmse_pred_test))\n",
    "\n",
    "print(\"R2 score test:\", r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "xgboost = xgb.XGBRegressor(objective=\"reg:squarederror\",random_state=42)\n",
    "\n",
    "#parameters for tuning\n",
    "# param_grid inspired by: https://github.com/yvasandvik/data-science-thesis/blob/master/method/xgb-tuning-model.ipynb\n",
    "params = {\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), \n",
    "    \"max_depth\": randint(2, 6),\n",
    "    \"n_estimators\": randint(100, 150)\n",
    "}\n",
    "#randomized search\n",
    "xgb_random = RandomizedSearchCV(xgboost, param_distributions=params, random_state=42, n_iter=10, cv=3, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# Train the model \n",
    "xgb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(xgb_random.best_estimator_)\n",
    "\n",
    "xgb_tuned = xgb_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = xgb_tuned.predict(X_valid)\n",
    "\n",
    "# Calculatig root mean squared error\n",
    "rmse_pred = mean_squared_error(y_valid, y_pred) \n",
    "\n",
    "print(\"Root Mean Absolute Error:\" , np.sqrt(rmse_pred))\n",
    "\n",
    "print(\"R2 score:\" , r2_score(y_valid, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set \n",
    "y_pred_test = xgb_tuned.predict(X_test)\n",
    "\n",
    "print(\"R2 score test:\" ,r2_score(y_test, y_pred_test))\n",
    "\n",
    "rmse_pred_test = mean_squared_error(y_test, y_pred_test) \n",
    "\n",
    "print(\"Root Mean Squared Error:\" , np.sqrt(rmse_pred_test))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### print(np.corrcoef(y_test,y_pred_test))\n",
    "\n",
    "r = np.corrcoef(y_test.T, y_pred_test.T)[0, 1]\n",
    "\n",
    "# Plot the true vs predicted values with some customizations\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('True vs. Predicted Values')\n",
    "plt.savefig(\"xgb_tunednormal.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the facecolor of the figure to white\n",
    "fig = plt.figure(facecolor='w')\n",
    "\n",
    "# Create a scatter histogram\n",
    "plt.hist2d(y_test, y_pred_test, bins=100, cmap='viridis',cmin=1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('True vs. Predicted Values')\n",
    "plt.savefig(\"xgbtuned.png\")\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datasets to evaluate each iteration\n",
    "evalset = [(X_train, y_train), (X_valid,y_valid)]\n",
    "\n",
    "# Fit the model\n",
    "xgb_tuned.fit(X_train, y_train, eval_metric='rmse', eval_set=evalset, verbose=False)\n",
    "\n",
    "# Evaluate performance\n",
    "yhat = xgb_tuned.predict(X_valid)\n",
    "score = mean_squared_error(y_valid, yhat)\n",
    "print('MSE: %.3f' % score)\n",
    "\n",
    "# Retrieve performance metrics\n",
    "results = clf_xgboost.evals_result()\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(results['validation_0']['rmse'], label='Training')\n",
    "plt.plot(results['validation_1']['rmse'], label='Validation')\n",
    "plt.xlabel('Number of Boosting Rounds')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('XGBRegressor Learning Curve')\n",
    "plt.legend()\n",
    "plt.savefig(\"xgb_val.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting important features for association with known motifs in Tomtom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance scores\n",
    "importances = xgb_tuned.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Get the top 1000 feature indices and their importance scores\n",
    "top_indices = indices[:1000]\n",
    "\n",
    "nucleotides= [feature_names[i] for i in indices]\n",
    "\n",
    "\n",
    "# Write selected nucleotides to a FASTA file\n",
    "with open(\"selected_features.fasta\", \"w\") as f:\n",
    "    for i, seq in enumerate(nucleotides):\n",
    "        f.write(\">seq{}\\n{}\\n\".format(i, seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "\n",
    "\n",
    "X_train,X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.3, random_state = 42)\n",
    "\n",
    "f_imp = pd.Series(.feature_importances_, index = X_names).sort_values(ascending=False)\n",
    "\n",
    "select= f_imp.iloc[0:200]\n",
    "\n",
    "selected_feat = list(select.index)\n",
    "\n",
    "\n",
    "# Select most important features\n",
    "important_indices = [X_names.index(feat) for feat in selected_feat]\n",
    "\n",
    "imp_train = X_train[:, important_indices]\n",
    "imp_valid = X_valid[:, important_indices]\n",
    "imp_test = X_test[:, important_indices]\n",
    "\n",
    "\n",
    "\n",
    "# Train the model \n",
    "xgb_tuned.fit(imp_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "y_pred_imp = xgb_tuned.predict(imp_valid)\n",
    "\n",
    "# Calculatig root mean absolute error\n",
    "rmse_pred_imp = mean_absolute_error(y_valid, y_pred_imp) \n",
    "\n",
    "print(\"Root Mean Absolute Error:\" , np.sqrt(rmse_pred_imp))\n",
    "\n",
    "print(\"R2 score:\" , r2_score(y_valid, y_pred_imp))\n",
    "\n",
    "\n",
    "# Predictions on test set \n",
    "y_pred_testimp = xgb_tuned.predict(imp_test)\n",
    "\n",
    "print(r2_score(y_test, y_pred_testimp))\n",
    "\n",
    "print(np.corrcoef(y_test,y_pred_testimp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterHarini",
   "language": "python",
   "name": "masterharini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
